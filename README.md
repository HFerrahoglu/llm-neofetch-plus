
<div align="center">

![image](assets/banner.png)

# ğŸš€ LLM-Neofetch++ 

![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)
![Python](https://img.shields.io/badge/python-3.8+-green.svg)
![License](https://img.shields.io/badge/license-MIT-orange.svg)
![Platform](https://img.shields.io/badge/platform-Windows%20|%20Linux%20|%20macOS-lightgrey.svg)

**Advanced System Information Tool for Local LLM Usage**

Show detailed hardware specs optimized for running local AI models

</div>

---

## âœ¨ Features

### ğŸ” **Comprehensive Hardware Detection**
- âœ… **CPU**: Model, cores, threads, frequency, temperature, usage
- âœ… **GPU**: NVIDIA (nvidia-smi), AMD (rocm-smi), Intel Arc detection
- âœ… **VRAM**: Total, used, and available video memory
- âœ… **RAM**: Physical memory and swap information
- âœ… **Storage**: Disk type (NVMe/SSD/HDD), capacity, speed benchmarks
- âœ… **Battery**: Charge level, power status, time remaining (laptops)
- âœ… **Apple Silicon**: M1/M2/M3/M4 detection with unified memory

### ğŸ¯ **Smart AI/LLM Features**
- ğŸ¤– **Model Recommendations**: Personalized suggestions based on your hardware
- ğŸ“Š **Quantization Guide**: GGUF formats explained (Q2_K through Q8_0)
- ğŸš€ **Backend Comparison**: Ollama, llama.cpp, vLLM, ExLlamaV2, LM Studio
- âš¡ **Performance Estimates**: Token/s predictions for different model sizes
- ğŸ’¡ **Optimization Tips**: Specific advice for your system configuration

### ğŸ¨ **Beautiful UI**
- ğŸŒˆ **Color-coded Output**: Easy to read with semantic colors
- ğŸ“Š **Progress Bars**: Visual representation of usage and capacity
- ğŸ”§ **Configurable**: Customize colors, emoji, detail level
- ğŸ“± **Responsive**: Adapts to terminal width

### ğŸ› ï¸ **Developer Friendly**
- ğŸ“¤ **Export Formats**: JSON, YAML, Markdown
- ğŸ§ª **Unit Tests**: Comprehensive test coverage
- ğŸ”Œ **Modular Design**: Easy to extend and customize
- ğŸ“ **Type Hints**: Full type annotations
- ğŸ› **Verbose Mode**: Detailed logging for debugging

---

## ğŸ“¦ Installation

### From Source (Recommended)

```bash
# Clone the repository
git clone https://github.com/HFerrahoglu/llm-neofetch-plus.git
cd llm-neofetch-plus

# Install dependencies
pip install -r requirements.txt

# Run directly
python llm_neofetch.py

# Or install globally
pip install -e .
llm-neofetch
```

### Using pip

```bash
pip install llm-neofetch-plus
llm-neofetch
```

---

## ğŸ® Usage

### Basic Usage

```bash
# Normal output (default)
llm-neofetch

# Minimal output
llm-neofetch -d 1

# Detailed output with all features
llm-neofetch -d 3

# Interactive mode (choose detail level)
llm-neofetch -i
```

### Advanced Usage

```bash
# Run disk benchmark (takes ~10 seconds)
llm-neofetch -b

# Export to different formats
llm-neofetch --export report.json      # JSON format
llm-neofetch --export report.yaml      # YAML format
llm-neofetch --export report.md        # Markdown format

# Verbose logging for debugging
llm-neofetch -v

# Custom config file
llm-neofetch --config /path/to/config.yaml

# Combine options
llm-neofetch -d 3 -b --export full_report.json
```

---

## ğŸ“¸ Screenshots

### Normal Output
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              âš¡ LLM â€¢ NEOFETCH ++  âš¡                                   â•‘
â•‘         Advanced System Info for Local LLM Usage                         â•‘
â•‘                    v1.0.0 â€¢ 2026 Edition                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ’» System Information
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  OS             Linux-6.5.0-1-amd64-x86_64-with-glibc2.38
  Kernel         6.5.0 (x86_64)
  Uptime         2d 14h 32m
  Python         3.11.5

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”§ CPU
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Model          AMD Ryzen 9 7950X 16-Core Processor
  Cores          16 physical / 32 threads
  Frequency      4200 MHz
  Usage          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]  35.2%

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ® GPU
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ğŸŸ¢ NVIDIA GeForce RTX 4090
      VRAM: 24.0 GB total
            [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 12.4/24.0 GB
      Usage          [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 20.0%
      Temp: 58Â°C

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ Personalized Model Recommendations
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  â–¸ Extra Large Models (70-72B)
    â€¢ Llama 3.1 70B
    â€¢ Qwen2.5 72B

  â–¸ Large Models (30-34B)
    â€¢ Llama 3.1 33B
    â€¢ Qwen2.5 32B
```

---

## âš™ï¸ Configuration

LLM-Neofetch++ uses a YAML configuration file. By default, it looks for:

1. `./config/config.yaml` (in the project directory)
2. `~/.config/llm-neofetch/config.yaml`
3. `/etc/llm-neofetch/config.yaml`

### Sample Configuration

```yaml
# UI Settings
ui:
  box_width: 76
  use_emoji: true
  show_progress_bars: true
  compact_mode: false

# Color Theme
colors:
  primary: "\033[1;34m"    # Blue
  success: "\033[1;32m"    # Green
  warning: "\033[1;33m"    # Yellow
  danger: "\033[1;31m"     # Red

# Performance Thresholds
thresholds:
  vram:
    excellent: 24  # GB
    good: 12
    moderate: 8
```

---

## ğŸ”§ Development

### Project Structure

```
llm-neofetch-plus/
â”œâ”€â”€ llm_neofetch.py          # Main application
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ detectors.py         # Hardware detection modules
â”‚   â””â”€â”€ ui.py                # UI rendering and formatting
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml          # Configuration file
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_all.py          # Unit tests
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ setup.py                 # Package setup
â””â”€â”€ README.md               # This file
```

### Running Tests

```bash
# Run all tests
python tests/test_all.py

# Run with pytest (if installed)
pytest tests/

# Run with coverage
pytest --cov=src tests/
```

### Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ¯ Use Cases

### For AI/ML Developers
- Quickly assess if your hardware can run specific models
- Get token/s estimates before downloading large models
- Understand which quantization format to use
- Optimize your LLM stack configuration

### For System Administrators
- Monitor system resources for AI workloads
- Export reports for documentation
- Benchmark storage performance for model loading
- Track GPU utilization and temperatures

### For Researchers
- Document hardware specs in papers
- Compare performance across different systems
- Generate reproducible system reports
- Share hardware configurations

---

## ğŸš€ Roadmap

- [ ] Docker container support
- [ ] Web dashboard (optional)
- [ ] Historical tracking and graphs
- [ ] Cloud GPU detection (AWS, GCP, Azure)
- [ ] LLM benchmarking suite
- [ ] Automatic model download suggestions
- [ ] Integration with popular LLM frameworks

---

## ğŸ¤ Acknowledgments

- Built with [psutil](https://github.com/giampaolo/psutil) for cross-platform system info
- Inspired by [neofetch](https://github.com/dylanaraps/neofetch)
- Community feedback from r/LocalLLaMA

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details

---

## ğŸŒŸ Star History

If you find this tool useful, please consider giving it a star â­

---

## ğŸ“ Support

- **Issues**: [GitHub Issues](https://github.com/HFerrahoglu/llm-neofetch-plus/issues)
- **Discussions**: [GitHub Discussions](https://github.com/HFerrahoglu/llm-neofetch-plus/discussions)
- **Email**: fhamz4@proton.me

---

<div align="center">

Made with â¤ï¸ for the Local LLM Community

</div>
